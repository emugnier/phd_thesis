
\section{The \dataset Dataset}\label{sec:benchmark}

Since existing Dafny benchmarks~\cite{sun2024clover,DafnyFSE24,loughridge2024dafnybench}
are limited to textbook-style problems and standalone functions,
we create a new benchmark dataset for assertion synthesis, which we dub \dataset.%
\footnote{Our dataset is available as part of the supplementary material,
and will be made public upon acceptance.}


\paragraph{Codebases}

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{chapters/laurel/fig/codesize.pdf}
  \caption{Number of lines per lemma for each codebase in \dataset.}
  \label{fig:codesize}
\end{figure}

We build the dataset from three Dafny codebases:
\begin{itemize}
  \item Dafny Libraries~\cite{LibrariesDafny} is
  a standard library of Dafny functions and lemmas
  useful for working with collections, non-linear arithmetic, file I/O, JSON, and more.
  \item \cedar Specification~\cite{Cedar} is a Dafny formalization of the Cedar authorization policy language used by AWS.%
  \footnote{A new version of Cedar has been released since the start of this project that uses Lean instead of Dafny.}
  \item \vmc~\cite{dafnyVMC} is a verified implementation of the Monte-Carlo algorithms and random number generators.
\end{itemize}
%
All three codebases
are maintained by professional software engineers at Amazon.
%
As shown in \autoref{tab:benchmark},
their size ranges from 4k to 18k lines of code,
containing between 219 and 2613 assertions which represent between
5.3\% and 10.8\% of the codebase.
Note that other hints such as loop invariants are barely used in these codebases
with only 108 loop invariants in total, representing 0.3\% of the lines of code.
%
The codebases are also diverse, targeting three different problem domains.


\begin{table}[]
  \caption{Codebases used to build \dataset}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{@{}lrrr|r@{}}
  \toprule
                    & \multicolumn{1}{c}{Libraries} & \multicolumn{1}{c}{\cedar} & \multicolumn{1}{c}{\vmc} & \multicolumn{1}{c}{Total} \\
\midrule
  LOC               & 18,316    & 10,674 & 4,064 & 33,054 \\
  Assertions        & 1234  (6.7\% of LOC )     & 1160  (10.8\% of LOC)  & 219  (5.3\% of LOC)  & 2,613 (7.9\% of LOC) \\
  Loop invariants  & 99 (0.5\% of LOC)           & 0         & 9 (0.2\% of LOC)      & 108 (0.3\% of LOC)       \\
  \midrule
  Extracted assertions & 58        & 54    & 33    & 145    \\
  \bottomrule
  \end{tabular}%
}
  \label{tab:benchmark}
\end{table}

\paragraph{Extracting Benchmark Assertions}

To determine if an assertion is correctly generated by \tool,
we need an oracle to judge its correctness and relevance.
%
One option is to use the existing assertion as ground truth,
considering the generated assertion correct
if it is syntactically identical to the existing one.
%
However, this approach is limited, as
there are often multiple ways to express the same assertion,
and our main objective is to assist the verifier in proving the lemma,
not to replicate existing assertions.
%
Instead, we rely on the verifier itself as an oracle:
%
we consider an assertion correctly generated
if, and only if, the verifier fails to prove the lemma without it.
%
Not all assertions in the codebase are directly necessary for the verifierâ€™s success.
%
To identify those that are essential,
we remove each assertion individually using a regular expression and re-verify the target lemma,
including the assertion into our dataset only if its removal causes the proof to fail.

Interestingly, we found a significant portion of assertions in these codebases were not actually necessary for proof completion
(though we caution that the table does not indicate all unnecessary assertions,
because if $n$ assertions from the same lemma can be removed one by one, it does not mean that all $n$ can be removed simultaneously).
%
While some of these unnecessary assertions prevent proof brittleness,
many seem to result from common development practices, where proof engineers try various assertions and may neglect to remove unneeded ones.
%
As these superfluous assertions can slow down verification,
 \tool offers a solution to automate assertion synthesis,
 ultimately producing cleaner, more efficient proofs.

\begin{figure}
\begin{dafny}[xrightmargin=0pt]
lemma SeqAddInequality<T>(s1: seq<T>, t1: T, s2: seq<T>, t2: T)
requires s1 != s2 || t1 != t2
ensures s1 + [t1] != s2 + [t2]
{
  if s1 == s2 {
    assert t1 != t2;
    var len := |s1|;
    assert (s1 + [t1])[len] != (s2 + [t2])[len];
  } else if |s1| == |s2| {
    var i :| 0 <= i < |s1| && s1[i] != s2[i];
    assert (s1 + [t1])[i] != (s2 + [t2])[i];
  }
}
\end{dafny}
\caption{A Dafny lemma with three assertions, out of which only two are \emph{assertions}, \ie necessary for the proof.}\label{fig:dataset-creation}
\end{figure}

Consider the Dafny lemma in \autoref{fig:dataset-creation},
%
which includes three assertions in Lines 4, 6, and 9.
%
We first remove the assertion in Line 4 and re-verify the lemma,
which leads to a verification failure;
therefore, this assertion/lemma pair is included in the dataset.
%
Next, we remove the assertion in Line 6,
but the proof succeeds with the remaining two assertions, so this assertion is excluded from the dataset.
%
Finally, we repeat the process for the assertion in Line 9,
which again leads to a verification failure, so it is included.

For each assertion,
we also check for exact duplicates within the same lemma.
%
If duplicates exist, we exclude the assertion from the dataset
to avoid overly simple synthesis tasks, where an identical assertion appears as an in-context example.

\paragraph{Assertions and Lemmas}
\autoref{tab:benchmark} shows the number of assertions extracted from each codebase,
yielding a total of \ntasks assertion synthesis tasks in \dataset.
%
This count represents only a fraction of the total assertions across the codebases,
as our regular expressions do not capture all possible assertion syntactic forms,
and duplicates are removed.
%
However, these assertions are integral to complex lemmas
that cannot be proven without them.


To assess the complexity of these assertions, we measured
the number of lines in the lemmas containing them as shown in \autoref{fig:codesize}.
%
Most lemmas exceed 10 lines of code,
with half spanning more than 18 lines of code,
%
indicating that they are non-trivial
%
and require significant effort to prove.