\subsection{Testing}
\label{sec:test}

\myquote
  {You really want to check
   that your spec is right,
   and one way to do that is testing.
  }
  {P7}

Verification and testing are often seen as ``orthogonal'' (P7),
and in the extreme, the former might be viewed as a replacement
for the latter, as formal verification is a way to ``test comprehensively
[…] on all inputs'' (P5).
%
However, in practice, \emph{all} participants that have engineered
substantial verified software reported that they have used some form
of testing along with verification.
%
Six participants
reported using testing as a way to \emph{compare the specification}
to some implementation or expected behavior.
%
Thus, to understand how developers test their specifications,
we first describe the two main \emph{goals} of verification,
and hence, testing, and then, the different \emph{techniques}
used to test during verified software development.

\subsection*{Goals}
%
We identified three kinds of testing goals pursued by participants. Some focused on \emph{verified specifications}; their projects aimed to build a reference model
of the deployed code, to formally establish 
desired properties of that model, and to then 
\emph{compare} the behaviors of the deployed code
(which is not verified \emph{per se}), to the reference model.
%
Others pursued \emph{verified implementations}, aiming to formally verify various properties of the
code that will actually be used in production to replace a legacy implementation. Here, the main
testing requirement was to make sure that this verified
implementation did not differ from the legacy (unverified) one. If any discrepancies arose, they needed to change the specification \emph{and} implementation, ensuring that the verified code had the same behavior as the legacy implementation.
%
Finally, six participants were focused on verifying new implementations of new specifications, which did not have any legacy compatibility requirements.

\mypara{Testing Goals.}
%
Compatibility is a common requirement. Most participants used testing 
to prevent the unpleasant surprises that occurred in the past
where, per P7, ``the [verified code] was wrong compared to the running code.''
%
However, the testing goals depended
on the goals of the project.
%
When the goal was to build a verified reference model,
P2, P4, and P9 reported doing \emph{conformance testing}, which ensured that the production implementation was equivalent
to the verified reference model.
%
Instead, when the goal was to build a verified implementation,
P5, P6 and P7 reported doing regression testing, comparing behavior in the legacy (unverified) system to that in the
new (verified) implementation.
P7 motivated this style of testing as a way to
``make sure to not break [clients]''.

Thus, regardless of whether they were building
verified specifications or verified implementations,
participants reported testing their code in an \emph{end-to-end}
fashion, to make sure either that the reference model
(or verified implementation) was equivalent to the
deployed code (or legacy code).
%
P4, P5, P7, P9, and P10 pointed out that such end-to-end
testing was especially important to gain trust.
%
P7 stated that ``shadow mode''---where the verified implementation
is run in parallel with the production code---was the ``gold standard''
and was ``what made people believe we could ship.''
%
One of the reasons, as P7 explained, is that even
after fuzzing the verified code extensively, they
found seven or eight differences between the production
answers and the verified code after months of running
in shadow mode, emphasizing the better coverage
of real data.
%
P10 went a step further: ``People don't care about our examples […]
they want to see real data in real data out.''

\mypara{Testing Assumptions and Performance.}
%
As part of testing, P7, P10, and P11 emphasized
the importance of using testing to validate any \emph{assumptions}
that arise from calls to external library code.
%
Such libraries are typically modeled in Dafny using \verb+:extern+
functions with trusted contracts.
%
P6 explained that the importance of testing
these contracts (in addition to reading the library code)
as ``sane contracts don't tell you if it actually models reality.''
%
Finally, in the verified implementation paradigm,
P7, P10, P11, and P12 reported using testing
to measure the \emph{performance} of the target code
generated by Dafny.
%
As the Dafny code is never run as such (\cref{sec:package}),
the performance aspect is something that can only be checked
post-transpiling as it depends on the target language.

\mypara{Testing Correctness of Transpilation.}
%
One participant reported that
%
``verifying the Dafny code doesn't help
  if there's a bug in the code generation.''
%
The participant ``had this little
subroutine, and I verified it perfectly true …
Fortunately I didn't just trust the verification.
I had some tests, too, and the test failed because
the code generation was wrong'' which could defeat the entire purpose of verification,
which doesn't ``actually prove that the executing code is correct.''

\subsection*{Testing Techniques}

The end-to-end testing was carried out using
a variety of \emph{existing} techniques.

\mypara{Fuzzing and Unit Testing.}
%
For example, P2, P5, P6, P7 described using
\emph{fuzzing} or \emph{property-based} testing \cite{Fink1997:Property}
and P2, P4 reported using unit tests.
%
P5 pointed out that the advantage of using these
techniques is that they produce well-established
metrics recognized in an industrial setting especially
``because there are a lot of company guidelines,
[for example] your commit needs to be covered by all tests.''
%
Additionally, P6, P12, and P13 explained also having
``unit tests which we write in Dafny and transpile
alongside the code,'' not as a substitute for
verification, but to ensure that the transpiling
did not break any properties of the Dafny code.

\mypara{Wrappers.}
%
However, P7 and P10 noted that end-to-end testing
is not the best fit to find problems with incorrect
libraries axioms as these kinds of tests are
expensive and provide poor error localization.
%
To help with that, P10 remarked that a recent feature can
``add dynamic checks […] at the interface boundaries,''
allowing code to fail fast when the assumptions are violated.
%
P6 knew about this feature but had not used it yet.
%
They explained: when there is a new feature they are
``never the first person to try it out'', illustrating
the difficulty of staying up to date with the tool.